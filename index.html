<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="Latent Sketchpad equips MLLMs with an internal visual scratchpad for multimodal reasoning.">
  <meta property="og:title" content="Latent Sketchpad: Sketching Visual Thoughts to Elicit Multimodal Reasoning in MLLMs"/>
  <meta property="og:description" content="We propose Latent Sketchpad, a framework that equips MLLMs with an internal visual scratchpad for multimodal reasoning."/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="Multimodal Large Language Models, Multimodal Reasoning, MLLM, Unified MLLM, Latent Reasoning">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Latent Sketchpad: Sketching Visual Thoughts to Elicit Multimodal Reasoning in MLLMs</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-2 publication-title"><span class="text-blue-italic">Latent Sketchpad</span> : Sketching Visual Thoughts to Elicit Multimodal Reasoning in MLLMs</h1>
            <div class="is-size-6 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://hwanyu112.github.io/" target="_blank">Huanyu Zhang</a><sup>*1,2,3,</sup>,</span>
                <span class="author-block">
                  <a href="https://scholar.google.com/citations?user=AwANOsQAAAAJ&hl=zh-CN" target="_blank">Wenshan Wu</a><sup>*1</sup>,</span>
                  <span class="author-block">
                    <a href="https://chengzu-li.github.io/" target="_blank">Chengzu Li</a><sup>4</sup></sup>,</span>
                  </span>
                  <span class="author-block">
                    Ning Shang<sup>1</sup></sup>,</span>
                  </span>
                  <span class="author-block">
                    Yan Xia<sup>1</sup></sup>,</span>
                  </span>
                  <span class="author-block">
                    Yangyu Huang<sup>1</sup></sup>,</span>
                  </span>
                  <span class="author-block">
                    Yifan Zhang<sup>2,3</sup></sup>,</span>
                  </span>
                  <span class="author-block">
                    Li Dong<sup>1</sup></sup>,</span>
                  </span>
                  <span class="author-block">
                    Zhang Zhang<sup>2,3</sup></sup>,</span>
                  </span>
                  <span class="author-block">
                    Liang Wang<sup>2,3</sup></sup>,</span>
                  </span>
                  <span class="author-block">
                    Tieniu Tan<sup>3,5</sup></sup>,</span>
                  </span>
                  <span class="author-block">
                    Furu Wei<sup>1</sup>
                  </span>
                  </div>

                  <div class="is-size-6 publication-authors" >
                    <span class="author-block">
                      <sup>1</sup> Microsoft Research <br>
                      <sup>2</sup>University of Chinese Academy of Sciences 
                      <sup>3</sup>Institute of Automation, Chinese Academy of Sciences <br>
                      <sup>4</sup>Language Technology Lab, University of Cambridge 
                      <sup>5</sup>Nanjing University
                      <!-- Conferance name and year -->
                    </span>
                    <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/2510.24514" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- Demo link -->
                    <span class="link-block">
                      <a href="https://huggingface.co/huanyu112/Latent-Sketchpad.Sketch_Decoder" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-download"></i>
                      </span>
                      <span>Sketch Decoder</span>
                    </a>
                  </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/hwanyu112/Latent-Sketchpad" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- Data Link -->
                <span class="link-block">
                  <a class="external-link button is-normal is-rounded is-light"
                    aria-disabled="true" title="Dataset will be released soon">
                    <span class="icon"><i class="fas fa-database"></i></span>
                    <span>Dataset · Coming soon</span>
                  </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser image-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="static/images/overview.png" alt="Teaser Image" style="width:100%; height:auto; border-radius:8px;">
      <h2 class="subtitle has-text-left" style="font-size:18px; line-height:1.5; font-family:'Noto Sans', sans-serif; max-width:1400px; margin:5px auto;">
        (a) Latent Sketchpad extends frontier MLLMs (e.g. Gemma3 and Qwen2.5-VL) to <mark>interleave text and visual latents generation</mark>, incorporating visual thoughts into reasoning.
        (b) The framework enables interleaved generation by equipping the pretrained MLLM with a <mark>Vision Head</mark> to generate visual latents autoregressively. A separately pretrained <mark>Sketch Decoder</mark> visualizes these latents into interpretable sketch images.
      </h2>
    </div>
  </div>
</section>
<!-- End teaser image -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            While Multimodal Large Language Models (MLLMs) excel at visual understanding, they often struggle in complex scenarios that require visual planning and imagination. 
            Inspired by how humans use sketching as a form of visual thinking to develop and communicate ideas, we introduce <span class="hl-blue bold">Latent Sketchpad, a framework that equips MLLMs with an internal visual scratchpad</span>.
            The internal visual representations of MLLMs have traditionally been confined to perceptual understanding.
            We repurpose them to support generative visual thought without compromising reasoning ability.
            <span class="hl-blue bold">Building on frontier MLLMs, our approach integrates visual generation directly into their native autoregressive reasoning process.</span>
            It allows the model to interleave textual reasoning with the generation of visual latents. 
            These latents guide the internal thought process and can be translated into sketch images for interpretability. 
            To realize this, we introduce two components: a Context-Aware Vision Head autoregressively produces visual representations, and a pretrained Sketch Decoder renders these into human-interpretable images.
            We evaluate the framework on our new dataset MazePlanning, where experiments across various MLLMs show that Latent Sketchpad delivers comparable or even superior reasoning performance.
            It further generalizes across distinct pretrained backbones, including Gemma3 and Qwen2.5-VL.
            By extending model's textual reasoning to visual thinking, our framework opens new opportunities for richer human–computer interaction and broader applications in areas such as education and design.
            <!-- While Multimodal Large Language Models (MLLMs) excel at visual understanding, they often struggle in complex scenarios that require visual planning and imagination. 
            Inspired by how humans use sketching as a form of visual thinking to develop and communicate ideas, we introduce <span class="hl-blue bold">Latent Sketchpad, a framework that equips MLLMs with an internal visual scratchpad</span>.
            Our approach <span class="hl-blue bold">integrates visual thinking directly into the MLLM's native generation process.</span> 
            It allows the model to interleave textual reasoning with the generation of visual latents. These latents act as abstract sketches that guide its internal thought process.  
            This is enabled by two lightweight components. A Context-Aware Vision Head autoregressively produces visual representations, while a pretrained Sketch Decoder renders these into human-interpretable images.
            Unlike prior methods that depend on external tools, Latent Sketchpad generates visual representations autonomously within the model. 
            To evaluate our framework, we introduce MazePlanning, a new dataset featuring complex, interleaved reasoning trajectories. 
            <span class="hl-blue bold">Experiments across diverse MLLMs, including Qwen2.5-VL and Gemma3, demonstrate the broad applicability of our method.</span>
            Latent Sketchpad repurposes the internal visual representations of MLLMs, which are traditionally used only for understanding, to support generative visual thought. 
            This unlocks new reasoning capabilities that extend far beyond language alone.
            By externalizing the model's thought process, our framework creates opportunities for richer human-computer interaction, and broader applications across domains such as education and design. -->
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- Video carousel -->
<!-- Video carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container has-text-centered">
      <h2 class="title is-3">Showcase of Latent Sketchpad on MazePlanning</h2>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-video1">
          <video poster="" id="video1" autoplay controls muted loop height="100%">
            <!-- Your video file here -->
            <source src="static/videos/maze_showcase1.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video2">
          <video poster="" id="video2" autoplay controls muted loop height="100%">
            <!-- Your video file here -->
            <source src="static/videos/maze_showcase2.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video3">
          <video poster="" id="video3" autoplay controls muted loop height="100%">\
            <!-- Your video file here -->
            <source src="static/videos/maze_showcase3.mp4"
            type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End video carousel -->


<!-- Image showcase -->
<section class="section">
  <div class="container">

    <!-- 卡片1 -->
    <div class="image-card box">
      <h3 class="title is-4 has-text-centered">Architecture of the Context-Aware Vision Head and Pretrained Sketch Decoder.</h3> <!-- 小标题 -->
      <figure class="image">
        <img src="static/images/model_architecture.png" alt="MY ALT TEXT"/>
      </figure>
      <p class="subtitle is-5 has-text-left mt-3">
        The Vision Head transforms hidden states from the MLLM backbone into visual latents. The Sketch Decoder operates independently, converting these latents into sketch-style images for visualization and interpretability.
      </p>
    </div>

    <!-- 卡片2 -->
    <div class="image-card box">
      <h3 class="title is-4 has-text-centered">Illustration of Generalization and Compatibility of the Pretrained Sketch Decoder</h3> <!-- 小标题 -->
      <figure class="image">
        <img src="static/images/visualizer.png" alt="MY ALT TEXT"/>
      </figure>
      <p class="subtitle is-5 has-text-left mt-3">
        (a) Quantitative reconstruction results (SSIM) across different vision encoders (OpenCLIP, Qwen2.5-VL and Gemma3) on unseen samples from MazePlanning dataset. 
        <br>
        (b) Qualitative examples of reconstructed sketches from visual latents produced by each encoder.
      </p>
    </div>

    <!-- 卡片3 -->
    <div class="image-card box">
      <h3 class="title is-4 has-text-centered">Visualizations from Latent Sketchpad-enhanced Gemma3 and Qwen2.5-VL</h3> <!-- 小标题 -->
      <figure class="image">
        <img src="static/images/task_visualization.png" alt="MY ALT TEXT"/>
      </figure>
    </div>

    <!-- 卡片4 -->
    <div class="image-card box">
      <h3 class="title is-4 has-text-centered">Performance Variation with Maze Size</h3> <!-- 小标题 -->
      <figure class="image">
        <img src="static/images/performance_size.png" alt="MY ALT TEXT"/>
      </figure>
    </div>

  </div>
</section>
<!-- End image showcase -->





<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>
        @article{zhang2025latentsketchpad,
          title={Latent Sketchpad: Sketching Visual Thoughts to Elicit Multimodal Reasoning in MLLMs},
          author={Zhang, Huanyu and Wu, Wenshan and Li, Chengzu and Shang, Ning and Xia, Yan and Huang, Yangyu and Zhang, Yifan and Dong, Li and Zhang, Zhang and Wang, Liang and Tan, Tieniu and Wei, Furu},
          journal={arXiv preprint arXiv:2510.24514},
          year={2025}
        }
      </code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            © 2025 Latent Sketchpad Authors ·
            <a href="https://github.com/hwanyu112/Latent-Sketchpad" target="_blank">Code</a> ·
            <a href="mailto:huanyu.zhang@cripac.ia.ac.cn">Contact</a> · <br>
            Licensed under
            <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">
              CC BY-SA 4.0
            </a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
